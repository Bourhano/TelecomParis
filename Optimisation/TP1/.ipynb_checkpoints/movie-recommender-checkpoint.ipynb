{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "\n",
    "##### By Bourhan DERNAYKA\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Presentation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "##### Question 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 1682)\n",
      "(943, 1682)\n"
     ]
    }
   ],
   "source": [
    "from movielens_utils import *\n",
    "\n",
    "path = \"./ml-100k/u.data\"\n",
    "data, mask = load_movielens(path, False)\n",
    "R = data\n",
    "print(R.shape)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer:\n",
    "   We clearly see that the size of the matrix R, which is the scores matrix is of size 943x1682. \n",
    "   \n",
    "   The minidata option is used to take just a small part of our full data and do the predictions on.\n",
    "    For instance, if we set \n",
    "   <code>minidata = True</code>\n",
    "we will get a matrix R and its Mask of sizes 100x200, meaning that the study now is being made on 100 users and just 200 movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DO NOT RUN THE FOLLOWING BLOCK IF YOU WANT THE STUDY TO BE MADE ON THE FULL DATA AND GET YOUR PREDICTION OF USER 300!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 200)\n"
     ]
    }
   ],
   "source": [
    "data, small_mask = load_movielens(path, True)\n",
    "R=data\n",
    "mask = small_mask\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "##### Question 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output of Question 1.1, we can see that the first dimension of R is 943, it means that our study is being held on 943 users.\n",
    "\n",
    "For the second dimension (movies), it is 1682. Meaning that the different movies watched by the different users is 1682 movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100 user, and 200 movie.\n"
     ]
    }
   ],
   "source": [
    "nUser, nMovie = R.shape\n",
    "print(\"There are\", nUser,\"user, and\", nMovie, \"movie.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of grades can be obtained by summing all of the matrix R elements. Since each element describes the presence of a grade put by a user on a movie. So it is a binary matrix.\n",
    "\n",
    "Here is the total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of grades is 3571 grade.\n"
     ]
    }
   ],
   "source": [
    "print(\"The total number of grades is\", sum(sum(mask)), \"grade.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 2. Finding P when Q$^0$ is fixed\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2.1\n",
    "\n",
    "    We modify in the library movielens_utils.py, the funcyion: objective.\n",
    "    After deriving the equation of g in relation with P using the definition of the derivative, the gradient of g(P) will look like:\n",
    "    \n",
    "$$ \\nabla g(P) = - Q^0  (1_k \\circ (R - Q^0 P)) + \\rho P$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "##### Question 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In order to extract the eigenvalues and vectors of the R matrix, we shall use a Singular Value Decomposition (SVD) method.\n",
    "\n",
    "    It will give us our initial P0 matrix, alongside with our -here supposed constant- matrix Q0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "F = [0, 1, 2, 3]\n",
    "singPointsNb = len(F)\n",
    "Q0, singular, P0 = svds(R, k=singPointsNb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 0.3\n",
    "val, gradP = objective(P0, Q0, R, mask, rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gvec(Pvec):\n",
    "    P = np.reshape(Pvec, P0.shape)\n",
    "    return objective(P, Q0, R, mask, rho)[0]\n",
    "def gradGvec(Pvec):\n",
    "    P = np.reshape(Pvec, P0.shape)\n",
    "    return objective(P, Q0, R, mask, rho)[1].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONLY RUN THE FOLLOWING BLOCK IF YOU ARE SEEING minData (you did run the cell that takes only a part of the data).\n",
    "#### Otherwise, if you have time, you can wait to check the correctness of the gradient :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004294649282380209"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import check_grad\n",
    "check_grad(Gvec, gradGvec, P0.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We run the objective function to calculate the value of the current equation with our chosen initial values, and the gradient of the function.\n",
    "\n",
    "    We check that the dimensions of the gradient complies with the definition of g, where P is our variable.\n",
    "\n",
    "    We then check the correctness of our calculated gradient with the check_grad() function of scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "##### Question 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We define here our objective function g(P), and its gradient in order to feed these functions to our gradient algorithm!\n",
    "\n",
    "    We are going to use the fixed step for this early version of the algorithm.\n",
    "    The step $\\gamma$ is equal to the inverse of the upper bound of our gradient function g.\n",
    "    In our case, $\\nabla g\\ (P)$ is upper bounded with $L_0 = \\rho + ||Q^0||_F^2$. Hence we find our fixed step size $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G(P):\n",
    "    return objective(P, Q0, R, mask, rho)[0]\n",
    "def gradG(P):\n",
    "    return objective(P, Q0, R, mask, rho)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(vect):\n",
    "    return np.sqrt(np.sum(np.square(vect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(g, P0, gamma, epsilon):\n",
    "    grad = gradG(P0)\n",
    "    P1 = P0 - gamma * grad\n",
    "    err = norm(grad)\n",
    "    print(\"error=\", err)\n",
    "    if err > epsilon:\n",
    "        P0 = P1\n",
    "        return gradient(g, P0, gamma, epsilon)\n",
    "    else:\n",
    "        return P1, err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "##### Question 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now we test our coded gradient method. It is recursive so we dont call it inside a loop, it will directly return the final answer.\n",
    "\n",
    "    The initiation 'point' is P0, which we got from the SVD decomposition.\n",
    "\n",
    "    We can check the error updates throughout the process in order to reach our preset precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma = 0.4347826086956521\n",
      "error= 181.96863512342927\n",
      "error= 99.55884573289072\n",
      "error= 57.513300335357\n",
      "error= 35.264228980586495\n",
      "error= 22.899159584671047\n",
      "error= 15.618960248777581\n",
      "error= 11.075111905179538\n",
      "error= 8.088615861266893\n",
      "error= 6.0415175797516545\n",
      "error= 4.591631407751436\n",
      "error= 3.538310363159769\n",
      "error= 2.757606872825926\n",
      "error= 2.1695266416786447\n",
      "error= 1.7205783158183117\n",
      "error= 1.3739560986316102\n",
      "error= 1.1037380696291479\n",
      "error= 0.8913105530796154\n",
      "\n",
      "The optimum value of g(P) (minimum), for an error correction limit of 1 is 11918.709116502863\n"
     ]
    }
   ],
   "source": [
    "gamma = 1 / (rho + norm(Q0.T.dot(Q0) ** 2))\n",
    "epsilon = 1\n",
    "print(\"Gamma =\", gamma)\n",
    "Popt, err = gradient(1, P0, gamma, epsilon)\n",
    "print(\"\\nThe optimum value of g(P) (minimum), for an error correction limit of\", epsilon, \"is\", G(Popt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "##### Question 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We need now to integrate the line search in our problem, in order to do so, we must implement the step size searching algorithm.\n",
    "    As explained in the course it is based on the following equation:\n",
    "$$ f(x^+(ba^l)) \\leq f(x_k) + <\\nabla f(x_k), x^+(ba^l) - x_k> + \\frac{1}{2ba^l}||x_k - x^+(ba^l)||^2 $$\n",
    "\n",
    "    where \n",
    "   $ x^+(\\gamma) = x_k - \\gamma \\nabla f(x_k)$, b > 0, and 0 < a < 1\n",
    "   \n",
    "   Our goal is to fiind the smallest nonnegative l such that the inequation up below holds!\n",
    "   hence, $ba^l$ will be our desired step size for the next iteration.\n",
    "   \n",
    "   As indicated in the course, we will use a = 0.5, and b a multiple of the original gamma as an initiation, and it then will be equal the 2 times the previous gamma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientWithLineSearch(gEqn, gradEqn, P0, a, b, l, epsilon):\n",
    "    grad = gradEqn(P0)\n",
    "    #line-search\n",
    "    Pk = P0 - b*a**l*grad\n",
    "    while gEqn(Pk) + 0.5*b*a**l*norm(grad)**2 - gEqn(P0) > 0:\n",
    "        print(\"            linesearch, l=\",l,\",gamma =\", b*a**l )\n",
    "        l += 1\n",
    "    print(\"   l was set to\",l,\",\", end=\" \")\n",
    "    gamma = b*a**l\n",
    "    print(\"the value of the step size (gamma) in this iteration is set to\", gamma)\n",
    "    P1 = P0 - gamma * grad\n",
    "    err = norm(grad)\n",
    "    print(\"current step error=\", err)\n",
    "    if err > epsilon:\n",
    "        P0 = P1\n",
    "        return gradientWithLineSearch(gEqn, gradEqn, P0, a, 2*gamma, 0, epsilon)\n",
    "    else:\n",
    "        return P1, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Gamma = 1.3043478260869563\n",
      "            linesearch, l= 0 ,gamma = 1.3043478260869563\n",
      "   l was set to 1 , the value of the step size (gamma) in this iteration is set to 0.6521739130434782\n",
      "current step error= 181.96863512342927\n",
      "            linesearch, l= 0 ,gamma = 1.3043478260869563\n",
      "   l was set to 1 , the value of the step size (gamma) in this iteration is set to 0.6521739130434782\n",
      "current step error= 61.16792028686352\n",
      "   l was set to 0 , the value of the step size (gamma) in this iteration is set to 1.3043478260869563\n",
      "current step error= 27.520830836421624\n",
      "            linesearch, l= 0 ,gamma = 2.6086956521739126\n",
      "            linesearch, l= 1 ,gamma = 1.3043478260869563\n",
      "            linesearch, l= 2 ,gamma = 0.6521739130434782\n",
      "            linesearch, l= 3 ,gamma = 0.3260869565217391\n",
      "   l was set to 4 , the value of the step size (gamma) in this iteration is set to 0.16304347826086954\n",
      "current step error= 7.691691473175599\n",
      "   l was set to 0 , the value of the step size (gamma) in this iteration is set to 0.3260869565217391\n",
      "current step error= 6.778417727868471\n",
      "   l was set to 0 , the value of the step size (gamma) in this iteration is set to 0.6521739130434782\n",
      "current step error= 5.27530602525042\n",
      "   l was set to 0 , the value of the step size (gamma) in this iteration is set to 1.3043478260869563\n",
      "current step error= 3.276966093039012\n",
      "            linesearch, l= 0 ,gamma = 2.6086956521739126\n",
      "   l was set to 1 , the value of the step size (gamma) in this iteration is set to 1.3043478260869563\n",
      "current step error= 1.364562378606396\n",
      "            linesearch, l= 0 ,gamma = 2.6086956521739126\n",
      "   l was set to 1 , the value of the step size (gamma) in this iteration is set to 1.3043478260869563\n",
      "current step error= 0.6334501966251365\n",
      "\n",
      "The optimum value of g(P) (minimum), for an error correction of 1 is 11918.20378089465\n"
     ]
    }
   ],
   "source": [
    "gamma0 = 1 / (rho + norm(Q0.T.dot(Q0) ** 2))\n",
    "a = 0.5; b = 3*gamma0; l = 0\n",
    "epsilon = 1\n",
    "gamma = b*a**l\n",
    "print(\"Initial Gamma =\", gamma)\n",
    "Popt, err = gradientWithLineSearch(G, gradG, P0, a, b, l, epsilon)\n",
    "print(\"\\nThe optimum value of g(P) (minimum), for an error correction of\", epsilon, \"is\", G(Popt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We note that our objective was minimized after the line search, we are now closer to a better minimum, hence able to make better predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 3. Resolution of the Full Problem\n",
    "      Finding optimal P and Q simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "##### Question 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f(P,Q) is our function to minimize, it os formed of 3 compartiments.\n",
    "The last 2 compartiments are the square of the Forbenius norm for the matrices P and Q simultaneously. These parts are equivalent to a 2nd degree equation. \n",
    "\n",
    "The first part, which is the squared norm of R-QP, is equivalent to a fourth order equation.\n",
    "In fact, we are multiplying the two variable matrices P and Q with each other. This create a second order polynomial, made with the variables of P and Q, and it is squared which makes it in the fourth order!\n",
    "\n",
    "The gradient of such a function will be of the same dimension of (P,Q) concatenated. That would be equivalent to a 3rd order equation in terms of P and Q.\n",
    "\n",
    "To prove such a  function Lipschitz continuous, we would need to look at its derivative, in our case it is the hessian of the original function F(P,Q). \n",
    "\n",
    "The hessian would be a 2nd order equation that is clearly not bounded! hence, gradF cannot be Lipschitz continuous!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "##### Question 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We shall now run the gradient algorithm on bounded functions, hence we proceed in the same way as we have done in the previous part of the exercice.\n",
    "\n",
    "    That means that we will take the best recommendations for P when Q is a constant, then use the optimum point Popt in the optimization algorithm that is searching for the optimal Q 'Qopt'\n",
    "\n",
    "    The result will be two matrices, that would be later on explained how to extract the good recommendation from.\n",
    "    \n",
    "    We begin by implementing the total_objective function of our library, then proceed with the previous optimal result Popt.\n",
    "    We now find Qopt using the same logic as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Gamma = 1.3043478260869563\n",
      "            linesearch, l= 0 ,gamma = 1.3043478260869563\n",
      "   l was set to 1 , the value of the step size (gamma) in this iteration is set to 0.6521739130434782\n",
      "current step error= 181.96863512342927\n",
      "            linesearch, l= 0 ,gamma = 1.3043478260869563\n",
      "   l was set to 1 , the value of the step size (gamma) in this iteration is set to 0.6521739130434782\n",
      "current step error= 61.16792028686352\n",
      "\n",
      "The optimum value of g(P) (minimum), for an error correction of 100 is 12485.00613899875\n"
     ]
    }
   ],
   "source": [
    "gamma0 = 1 / (rho + norm(Q0.T.dot(Q0) ** 2))\n",
    "a = 0.5; b = 3*gamma0; l = 0\n",
    "epsilon = 100\n",
    "gamma = b*a**l\n",
    "print(\"Initial Gamma =\", gamma)\n",
    "Popt, err = gradientWithLineSearch(G, gradG, P0, a, b, l, epsilon)\n",
    "print(\"\\nThe optimum value of g(P) (minimum), for an error correction of\", epsilon, \"is\", G(Popt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(Q):\n",
    "    return total_objective(Popt, Q, R, mask, rho)[0]\n",
    "def gradF(Q):\n",
    "    return total_objective(Popt, Q, R, mask, rho)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Gamma = 4.1839593081887375e-07\n",
      "   l was set to 0 , the value of the step size (gamma) in this iteration is set to 4.1839593081887375e-07\n",
      "current step error= 9504.719701592723\n",
      "   l was set to 0 , the value of the step size (gamma) in this iteration is set to 8.367918616377475e-07\n",
      "current step error= 9461.732234546911\n",
      "   l was set to 0 , the value of the step size (gamma) in this iteration is set to 1.673583723275495e-06\n",
      "current step error= 9376.308668255539\n",
      "   l was set to 0 , the value of the step size (gamma) in this iteration is set to 3.34716744655099e-06\n",
      "current step error= 9207.652518208915\n",
      "   l was set to 0 , the value of the step size (gamma) in this iteration is set to 6.69433489310198e-06\n",
      "current step error= 8878.988937048032\n",
      "   l was set to 0 , the value of the step size (gamma) in this iteration is set to 1.338866978620396e-05\n",
      "current step error= 8255.345830114557\n",
      "   l was set to 0 , the value of the step size (gamma) in this iteration is set to 2.677733957240792e-05\n",
      "current step error= 7135.672973879306\n",
      "   l was set to 0 , the value of the step size (gamma) in this iteration is set to 5.355467914481584e-05\n",
      "current step error= 5352.0564877777715\n",
      "   l was set to 0 , the value of the step size (gamma) in this iteration is set to 0.00010710935828963168\n",
      "current step error= 3194.6430931381715\n",
      "   l was set to 0 , the value of the step size (gamma) in this iteration is set to 0.00021421871657926336\n",
      "current step error= 1690.6867588165003\n",
      "            linesearch, l= 0 ,gamma = 0.0004284374331585267\n",
      "            linesearch, l= 1 ,gamma = 0.00021421871657926336\n",
      "            linesearch, l= 2 ,gamma = 0.00010710935828963168\n",
      "   l was set to 3 , the value of the step size (gamma) in this iteration is set to 5.355467914481584e-05\n",
      "current step error= 1170.4434458242647\n",
      "   l was set to 0 , the value of the step size (gamma) in this iteration is set to 0.00010710935828963168\n",
      "current step error= 974.5644905282699\n",
      "\n",
      "The optimum value of f(Q) (minimum), for an error correction of 1000 is 6865.1767618320155\n"
     ]
    }
   ],
   "source": [
    "gamma0 = 1 / (rho + norm(Popt.T.dot(Popt) ** 2))\n",
    "a = 0.5; b = 2*gamma0; l = 0\n",
    "epsilon = 1000\n",
    "gamma = b*a**l\n",
    "print(\"Initial Gamma =\", gamma)\n",
    "Qopt, err = gradientWithLineSearch(F, gradF, Q0, a, b, l, epsilon)\n",
    "print(\"\\nThe optimum value of f(Q) (minimum), for an error correction of\", epsilon, \"is\", F(Qopt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "VERIFICTAIONS AND TESTS /// IGNORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "PQvec = np.concatenate([P0.ravel(), Q0.ravel()])\n",
    "def F(PQvec):\n",
    "    return total_objective_vectorized(PQvec, R, Mask, rho)[0]\n",
    "def gradF(PQvec):\n",
    "    return total_objective_vectorized(PQvec, R, Mask, rho)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullGradientWithLineSearch(fEqn, gradEqn, QP0, a, b, l, epsilon):\n",
    "    grad = gradEqn(QP0)\n",
    "    #line-search\n",
    "    QPk = QP0 - b*a**l*grad\n",
    "    while fEqn(QPk) + 0.5*b*a**l*norm(grad)**2 - fEqn(QP0) > 0:\n",
    "        print(fEqn(QPk) + 0.5*b*a**l*norm(grad)**2 - fEqn(QP0))\n",
    "        print(\"         linesearch, l=\",l,\",gamma =\", b*a**l )\n",
    "        l += 1\n",
    "    print(\"akal 3al\", fEqn(QPk) + 0.5*b*a**l*norm(grad)**2 - fEqn(QP0),\"l=\",l)\n",
    "    gamma = b*a**l\n",
    "    print(gamma)\n",
    "    QP1 = QP0 - gamma * grad\n",
    "    err = norm(grad)\n",
    "    print(\"error=\", err)\n",
    "    if err > epsilon:\n",
    "        QP0 = QP1\n",
    "        return fullGradientWithLineSearch(fEqn, gradEqn, QP0, a, 2*gamma, 0, epsilon)\n",
    "    else:\n",
    "        return QP1, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullGradientWithLineSearch(fEqn, gradEqn, QP0, gamma, epsilon):\n",
    "    grad = gradEqn(QP0)\n",
    "    QP1 = QP0 - gamma * grad\n",
    "    err = norm(grad)\n",
    "    print(\"error=\", err)\n",
    "    if err > epsilon:\n",
    "        QP0 = QP1\n",
    "        return fullGradientWithLineSearch(fEqn, gradEqn, QP0, gamma, epsilon)\n",
    "    else:\n",
    "        return QP1, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma0 = 1 / (rho + norm(PQvec.T.dot(PQvec)) ** 2)\n",
    "a = 0.5; b = 2*gamma0; l = 1\n",
    "epsilon = 100\n",
    "gamma = b*a**l\n",
    "gamma = 0.00001\n",
    "print(\"Gamma =\", gamma)\n",
    "PQopt, err = fullGradientWithLineSearch(F, gradF, PQvec, gamma, epsilon)\n",
    "print(\"\\nThe optimum value of g(P) (minimum), for an error correction of\", epsilon, \"is\", F(PQopt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "##### Question 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Based on the logic of our algorithm, the product of the matrices P and Q will be a prediction of our initial values-missing matrix R, which includes the likely grades the users may set to certain movies.\n",
    "    \n",
    "    Hence, our predictor matrix will be Rp = Qopt.Popt which represents what each user(line) would grade the corresponding movie(column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userId = 299\n",
    "Rp = Qopt.dot(Popt) * mask\n",
    "user300 = Rp[userId-1]\n",
    "recommendations = {}\n",
    "for i in range(len(user300)):\n",
    "    if user300[i] !=0 :\n",
    "        recommendations[i] = user300[i]\n",
    "recs = recommendations\n",
    "recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"The movie of id\", recommendations[0][0] + 1,\"is recommended to the user\", userId, \"\\nHe will most probably highly grade it comparing it to other options!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"here is the full list of movies suggested to user\", userId, \"by increasing order:\")\n",
    "{k: v for k, v in sorted(recs.items(), key=lambda item: item[1])}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
