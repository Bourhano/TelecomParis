{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Predictor\n",
    "\n",
    "##### By Bourhan DERNAYKA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 1. Data acquisition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from breastcancer_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "filename = 'wdbc_M1_B0.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 569 different entry, with 30 parameter each.\n"
     ]
    }
   ],
   "source": [
    "X, y = load_breastcancer(filename)\n",
    "n, m = X.shape\n",
    "print(\"There are\", n, \"different entry, with\", m, \"parameter each.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = y.reshape(y.shape[0],1)   this line solves some calculation problems where y might be considered without second dimension,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 2. Subgradient Method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Question 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Our objective function is the minimization of two sums, the first being half of the sum of the squared elements of the prediction vector $v \\in \\mathbb{R}^m$:\n",
    "$$\\frac{1}{2}\\sum_{j=1}^m{v_j^2}$$ \n",
    "    and the second is the sum of the elements of the conditional vector $\\xi \\in \\mathbb{R}^n$ multiplied by c. \n",
    "\n",
    "These conditions are that each $\\xi_i$ for i in [1,n] should be greater than or equal to:\n",
    "\n",
    "- $\\xi_i \\geq 0$\n",
    "\n",
    "- $\\xi_i \\geq 1 - y_i (x_i^T v + a)$\n",
    "\n",
    "    So $\\xi_i$ must qualify both conditions at the same time, so the minimum it is permitted to be will be the maximum between these two conditions in order to fulfill them concurrently.\n",
    "    Hence, we can replace all $\\xi_i$ for i in [1,n] by the max() function of the two conditions, so we get :\n",
    "    $$ objective = \\frac{1}{2}\\sum_{j=1}^m{v_j^2} + c \\sum_{i=1}^n{max(0, 1 - y_i (x_i^T v + a)} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Question 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23a93d38940>,\n",
       " <matplotlib.lines.Line2D at 0x23a93d388d0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z1 = np.linspace(-5, 1, 50)\n",
    "z2 = np.linspace(1, 5, 50)\n",
    "z = np.hstack([z1, z2])\n",
    "h = np.hstack([1-z1, z2*0])\n",
    "dh = np.hstack([-1*np.ones(len(z1)), z2*0])\n",
    "plt.plot(z,h,'+', z,dh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   The $h(z)$ function is defined by parts.<br> \n",
    "   We can see in blue the $h(z)$ function, it is descendant in the first part, constant in the second part.\n",
    "   <br>Hence, its derivative is negative in the first part, null in the second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Question 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ f(v, a) = \\frac{1}{2}\\sum_{j=1}^m{v_j^2} + c \\sum_{i=1}^n{max(0, 1 - y_i (x_i^T v + a)} $$ \n",
    " \n",
    "We set $ N(v,a) = \\frac{1}{2}\\sum_{j=1}^m{v_j^2} $ a function dependant of the vector $v \\in \\mathbb{R}^m$\n",
    " <br>and\n",
    " \n",
    " <span>$ H(M(v,a)) = \\sum_{i=1}^n{max(0, 1 - y_i (x_i^T v + a)\\ )}$<br>   \n",
    " Where the function $ H(t)\\ =\\ \\sum_{i=1}^n{max(0, 1 - t_i)}\\ \\ $\n",
    "    and $M(v,a)$ is a vector of length n.\n",
    "    \n",
    "In fact, $M$ is a linear application applied on the elements of the vector $(v,a) \\in \\mathbb{R}^{m+1}$, \n",
    "    since $ M_{i={1..n}}(v,a) = y_i (x_i^T v + a)$, we can write:\n",
    "    $$ M_{i={1..n}}(v,a) = y^T (x_i^T . v + a) = y^T ({xe}_i^T . va ) $$\n",
    "    \n",
    "where ${xe}_i \\in \\mathbb{R}^{m+1}$ and $va = (v,a) \\in \\mathbb{R}^{m+1}$\n",
    "    \n",
    "  So, we can conclude that $ M = Y . X_e \\in \\mathbb{R}^{nxm+1}$ where $Y$ is a n-diagonal matrix of $y_i$ and $X_e$ is the augmented matrix in $\\mathbb{R}^{nxm+1}$ with the last column made of ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Hence we have $ f(v, a) = N(v,a) + c H(M\\ .\\ va) $, a separable function.\n",
    "\n",
    "Its derivative will be:  $ \\partial f(v, a) = \\partial N(v,a) + c\\ M^T \\partial H(M(v,a))\\ $ using the derivative formula of a function with linear application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\partial N(t) = t$ $\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $where $t \\in \\mathbb{R}^m+1$\n",
    "\n",
    "$\\partial H(t) = \\partial h(t_1)\\ x\\ ...\\ x\\  \\partial h(t_{m+1}) $ $\\ \\ \\ \\ \\ \\ $ where $\\partial h$ is defined in Question 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Question 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = np.ones((n,1))\n",
    "Xe = np.hstack((X, ones))\n",
    "M = np.diag(y).dot(Xe)\n",
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(v, a):\n",
    "        \"\"\"\n",
    "            returns the objective function value for the specified inputs\n",
    "            Note: c must be defined preliminary, \n",
    "                    along with X (the covariates matrix), \n",
    "                    and y the output vector of our data.\n",
    "        \"\"\"\n",
    "        N = np.sum(v ** 2)/2\n",
    "        Mva = np.ones(n) - y * (np.dot(X, v) + a)\n",
    "        H = sum(Mva * (Mva>0)) \n",
    "        obj = N + c * H\n",
    "        \n",
    "        ones = np.ones((n,1))\n",
    "        Xe = np.hstack((X, ones))\n",
    "        M = np.diag(y).dot(Xe)\n",
    "        dH = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            if Mva[i] > 0:\n",
    "                dH[i] = -1\n",
    "        dN = np.hstack((v, 0)).T\n",
    "        grad = dN + c * M.T.dot(dH)\n",
    "        \n",
    "        return obj, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(603.8252882581148,\n",
       " array([-38.73829648, -20.70475549, -34.49379607, -34.9876155 ,\n",
       "         38.59618269,  45.35166232,  23.55585667,   1.95675574,\n",
       "         36.5927392 ,  89.42902745,  -6.13062046,  28.6170554 ,\n",
       "          0.18689247, -13.53182243,  59.92957203,  84.51089819,\n",
       "         72.68924277,  60.05022852,  41.47511889,  99.88454603,\n",
       "        -39.4982886 , -27.61455329, -34.31830559, -34.37001474,\n",
       "         18.65124737,  28.15298039,  20.26382933,  -2.07793185,\n",
       "         -2.28514378,  55.68259705,  26.        ]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = np.ones(m)\n",
    "a = 1; c = 1\n",
    "objective(V, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a neat way to build the objective: #ignore#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "603.8252882581148"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = np.sum(V ** 2)/2\n",
    "Xe = np.hstack((X, ones))\n",
    "Va = np.hstack((V, a))\n",
    "Mva = 1 - y * np.dot(Xe, Va)\n",
    "M = (Xe.T * y).T\n",
    "H = sum(Mva * (Mva>0))\n",
    "f = N + c * H\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Question 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(vect):\n",
    "    return np.sqrt(np.sum(np.square(vect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(V, a):\n",
    "    Va0 = np.hstack((V, a))\n",
    "    mod0 = np.inf \n",
    "    for k in range(0,10000):\n",
    "        gamma = 1 / (k + 1)\n",
    "        grad = objective(Va0[:-1], Va0[-1])[1]\n",
    "        mod = norm(grad)\n",
    "        #print(mod)\n",
    "        Va  = Va0 - gamma * grad\n",
    "        Va0 = Va\n",
    "        \n",
    "        mod0 = mod\n",
    "    return Va[:-1], Va[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Test with zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569.0, array([-401.67227502, -228.44097367, -408.60883936, -390.09318973,\n",
      "       -197.28489321, -328.22148127, -383.14721063, -427.3041985 ,\n",
      "       -181.84509798,    7.0634352 , -312.04525371,    4.56861423,\n",
      "       -305.99668108, -301.64736575,   36.873181  , -161.21243374,\n",
      "       -139.60579724, -224.51080955,    3.58836467,  -42.90155493,\n",
      "       -427.21613038, -251.39455908, -430.77071337, -403.76117734,\n",
      "       -231.89607933, -325.17580208, -362.92710448, -436.63153222,\n",
      "       -229.05116772, -178.19917556,  145.        ]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(26.58455025371073,\n",
       " array([ 2.03063272, -0.9611799 ,  1.61834199,  2.04766806, -4.84796227,\n",
       "        -5.10335695, -3.57037182, -2.33589113, -5.12372002, -4.83122008,\n",
       "         2.82118474, -1.33184844,  1.56231412,  1.99997122, -3.18614696,\n",
       "        -6.10099066, -5.66910111, -3.80741963, -4.55455713, -3.22669201,\n",
       "         1.84103496, -1.94109599,  1.20020529,  1.71995579, -4.02554551,\n",
       "        -4.50742006, -4.52611778, -3.21616738, -4.1301077 , -3.61746869,\n",
       "        -2.        ]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V0 = np.zeros(m); a0 = 0\n",
    "print(objective(V0, a0))\n",
    "Vopt, aopt = gradient(V0, a0)\n",
    "opt = objective(Vopt, aopt)[0]\n",
    "opt\n",
    "objective(Vopt, aopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Test with ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(603.8252882581148, array([-38.73829648, -20.70475549, -34.49379607, -34.9876155 ,\n",
      "        38.59618269,  45.35166232,  23.55585667,   1.95675574,\n",
      "        36.5927392 ,  89.42902745,  -6.13062046,  28.6170554 ,\n",
      "         0.18689247, -13.53182243,  59.92957203,  84.51089819,\n",
      "        72.68924277,  60.05022852,  41.47511889,  99.88454603,\n",
      "       -39.4982886 , -27.61455329, -34.31830559, -34.37001474,\n",
      "        18.65124737,  28.15298039,  20.26382933,  -2.07793185,\n",
      "        -2.28514378,  55.68259705,  26.        ]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(26.53689555318948,\n",
       " array([ 0.18195168,  0.94054712,  0.35516952, -0.15205002,  0.8244559 ,\n",
       "         3.21611425,  0.86139221,  0.64485046,  2.52556414,  2.07984036,\n",
       "        -1.63198288, -0.26969245, -1.07659087, -0.97133772,  0.63077604,\n",
       "         5.18433793,  0.55072144, -0.215716  ,  3.33709558,  2.52509351,\n",
       "         0.20728638,  1.40391121,  0.52276148, -0.15571956,  1.82368762,\n",
       "         4.84113034,  1.8431172 ,  1.22342048,  5.83732042,  3.90970319,\n",
       "         2.        ]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V0 = np.ones(m); a0 = 1\n",
    "print(objective(V0, a0))\n",
    "Vopt, aopt = gradient(V0, a0)\n",
    "opt = objective(Vopt, aopt)[0]\n",
    "objective(Vopt, aopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "##### Extra: Gradient function accuracy verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gvec(Va):\n",
    "    return objective(Va[:-1], Va[-1])[0]\n",
    "def gradGvec(Va):\n",
    "    return objective(Va[:-1], Va[-1])[1].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the gradient error for initializationvalues of 1 is: 7.280418426132196e-05\n",
      "the gradient error for initializationvalues of 0 is: 0.00014103374334746124\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import check_grad\n",
    "V0 = np.ones(m); a0 = 1\n",
    "err = check_grad(Gvec, gradGvec, np.hstack((V0, a0)))\n",
    "print(\"the gradient error for initializationvalues of 1 is:\", err)\n",
    "V0 = np.zeros(m); a0 = 0\n",
    "err = check_grad(Gvec, gradGvec, np.hstack((V0, a0)))\n",
    "print(\"the gradient error for initializationvalues of 0 is:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, our gradient function representation is accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 3. Stochastic Subgradient Method \n",
    "\n",
    "#### Question 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that a function $f_I$ that has a random variable parameter $I \\sim U[1,n]$, has an expectance of $f_I * uniform\\ distribution = f_I * 1/n$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ f_i(v, a) = \\frac{1}{2}\\sum_{j=1}^m{v_j^2} + cn {max(0, 1 - y_i (x_i^T v + a))} $$\n",
    " \n",
    " $ \\mathbb{E}{[f_i(v, a)] = \\mathbb{E}[\\frac{1}{2}\\sum_{j=1}^m{v_j^2} + cn *{max(0, 1 - y_i (x_i^T v + a))}]}\\\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \n",
    "                         = \\frac{1}{2}\\sum_{j=1}^m{v_j^2} + cn * \\mathbb{E}[{max(0, 1 - y_i (x_i^T v + a))}]\\\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \n",
    "                         = \\frac{1}{2}\\sum_{j=1}^m{v_j^2} + cn *\\sum_{k=1}^n{ \\frac{1}{n} max(0, 1 - y_k (x_k^T v + a))]}\\\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \n",
    "                         = \\frac{1}{2}\\sum_{j=1}^m{v_j^2} + c n/n *\\sum_{i=1}^n{max(0, 1 - y_i (x_i^T v + a)}\n",
    "                         \\\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = f(v,a)\n",
    " $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Question 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f_i(v, a) = \\frac{1}{2}\\sum_{j=1}^m{v_j^2} + cn {max(0, 1 - y_i (x_i^T v + a)} $$\n",
    "\n",
    "$\\partial f_i(v, a) = \\frac{1}{2}\\partial{\\sum_{j=1}^m{v_j^2}} + cn *\\partial{{max(0, 1 - y_i (x_i^T v + a))}}\\\\$\n",
    "$\\partial f_i(v, a) = \\frac{1}{2}\\partial{\\|\\| v\\|\\|_2^2} + cn *\\prod _{j=1}^{m+1} \\partial{h_j(y_i (x_i^T v + a))}\\\\ $\n",
    "\n",
    "$\\partial f_i(v, a) = \\binom{v}{0} + cn *\\prod _{j=1}^{m+1} \\partial{h_j(y_i (x_i^T v + a))}$ where the $\\partial h_j$ are those defined in question 2.3\n",
    "\n",
    "$\\partial f_i(v, a) = \\binom{v}{0} + cn * [\\ \\  -1\\ $ if $y_i (x_i^T v + a) < 1,\\ \\ \\ \\ \\  0\\ $if $y_i (x_i^T v + a) > 1,\\ \\ \\ \\ \\  [-1, 0]$ else$\\ \\ ]$\n",
    "\n",
    "We note that $\\partial f_i(v, a)$ is in $\\mathbb{R}^{m+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Question 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_i(v, a, i):\n",
    "        \"\"\"\n",
    "            returns the objective function value and the gradient for the specified inputs\n",
    "                    taking into consideration the i_th sample.\n",
    "            Note: c must be defined preliminary, \n",
    "                    along with X (the covariates matrix), \n",
    "                    and y the output vector of our data.\n",
    "        \"\"\"\n",
    "        N = np.sum(v ** 2)/2\n",
    "        Mva = 1 - y[i] * (np.dot(X[i], v) + a)\n",
    "        H = Mva * (Mva>0) \n",
    "        obj = N + c * H\n",
    "        \n",
    "        ones = np.ones(1)\n",
    "        Xe = np.hstack((X[i], ones))\n",
    "        M = y[i]*Xe\n",
    "        dH = np.zeros(1)\n",
    "        if Mva > 0:\n",
    "                dH[0] = -1\n",
    "        dN = np.hstack((v, 0)).T\n",
    "        grad = dN + c * dH * M.T\n",
    "        \n",
    "        return obj, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientStat(V, a):\n",
    "    Va0 = np.hstack((V, a))\n",
    "    mod0 = np.inf \n",
    "    for k in range(0,10000):\n",
    "        i = int(np.random.uniform(0,n))\n",
    "        gamma = 1 / (k + 1)\n",
    "        grad = objective_i(Va0[:-1], Va0[-1], i)[1]\n",
    "        mod = norm(grad)\n",
    "        #print(mod)\n",
    "        Va  = Va0 - gamma * grad\n",
    "        Va0 = Va\n",
    "        \n",
    "        mod0 = mod\n",
    "    return Va[:-1], Va[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569.0, array([-401.67227502, -228.44097367, -408.60883936, -390.09318973,\n",
      "       -197.28489321, -328.22148127, -383.14721063, -427.3041985 ,\n",
      "       -181.84509798,    7.0634352 , -312.04525371,    4.56861423,\n",
      "       -305.99668108, -301.64736575,   36.873181  , -161.21243374,\n",
      "       -139.60579724, -224.51080955,    3.58836467,  -42.90155493,\n",
      "       -427.21613038, -251.39455908, -430.77071337, -403.76117734,\n",
      "       -231.89607933, -325.17580208, -362.92710448, -436.63153222,\n",
      "       -229.05116772, -178.19917556,  145.        ]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100.69762726859383,\n",
       " array([-58.34663663, -45.78150406, -57.41785199, -55.12050486,\n",
       "        -13.54547848, -16.16568732, -40.19270358, -56.20706932,\n",
       "        -17.99835056,  34.38889218, -44.11730657,  13.12653542,\n",
       "        -37.03075641, -39.58328031,   1.14512843,  13.34418279,\n",
       "         20.311828  ,  -3.04077967,   9.92418419,  33.04692096,\n",
       "        -68.61882267, -57.59339171, -65.74135029, -61.50946002,\n",
       "        -49.44887342, -37.08885973, -43.27012476, -64.49290905,\n",
       "        -50.91092635, -18.8101674 , -12.        ]))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import uniform\n",
    "i = int(np.random.uniform(0,n))\n",
    "\n",
    "V0 = np.zeros(m); a0 = 0\n",
    "print(objective(V0, a0))\n",
    "Vopt, aopt = gradientStat(V0, a0)\n",
    "opt = objective_i(Vopt, aopt, i)[0]\n",
    "objective(Vopt, aopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 4. Augmented Lagrangian Method \n",
    "\n",
    "#### Question 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem (1) is conditionned by two inequalities, we will transform them to the following shape: \n",
    "\n",
    "1) $$- \\xi \\leq 0 $$\n",
    "\n",
    "2) $$ - \\xi_i + 1 - y_i (x_i^T v + a) \\leq 0$$\n",
    "\n",
    "\n",
    "Hence, the associated Langrangianof Problem (1) is:\n",
    "$$ \n",
    "    L(v, a, \\xi, \\phi, \\psi) = \\frac{1}{2}\\| v\\|_2^2 + c \\sum_{i=1}^n{\\xi_i} - \\xi . \\phi + \\sum_{j=1}^n{ \\psi_j . (- \\xi_j + 1 - y_j (x_j^T v + a))}\n",
    "$$\n",
    "\n",
    "where $\\phi , \\psi \\in \\mathbb{R}^n$\n",
    "\n",
    "So, we note that we have $2n$ Lagrange equation variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Question 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
