{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generally Important libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Libraries for Gridsearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Libraries for PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Libraries imported for the Decision Tree Part\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "#Libraries imported for the Random forest part\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Libraries for SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Libraries for MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Keras libraries for Neural Networks\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from keras import optimizers, initializers\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. remove sparse data\n",
    "# 2. Graph everything\n",
    "# 3. Comments and analysis\n",
    "# 4. Test different classifiers?\n",
    "# 5. ACCURACY! 95 minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 784), (1000, 784))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('G_3_ant.npy').shape, np.load('G_3_grapes.npy').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1000 image of 28*28 size = 784.\n",
    "\n",
    "We have p=784 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ant = np.load('G_3_ant.npy')\n",
    "n_ant = X_ant.shape[0]\n",
    "y_ant = np.ones(n_ant)\n",
    "X_grapes = np.load('G_3_grapes.npy')\n",
    "n_grapes = X_ant.shape[0]\n",
    "y_grapes = -1 * np.ones(n_grapes)\n",
    "X = np.vstack([X_ant, X_grapes])\n",
    "y = np.hstack([y_ant, y_grapes])\n",
    "X, y = shuffle(X, y, random_state=1)\n",
    "X = X / 255.\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "test score: 0.756\n",
      "Fitting 5 folds for each of 154 candidates, totalling 770 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 770 out of 770 | elapsed: 10.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n",
      "{'C': 10.0, 'gamma': 0.001, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel=\"linear\")\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"test score:\", score)\n",
    "grid = {'C' : np.logspace(0, 6, 7),\n",
    "        'gamma' : np.linspace(0.001,0.01,11),\n",
    "        'kernel':['poly', 'rbf']\n",
    "       }\n",
    "clf = GridSearchCV(estimator = SVC(), param_grid = grid, n_jobs=-1, verbose=2)\n",
    "clf.fit(X,y)\n",
    "print(clf.best_score_)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "Test score: 0.842\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the classifier to the training set\")\n",
    "clf = SVC(kernel=\"linear\", C=0.001)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"Test score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[206  23]\n",
      " [ 19 252]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9593333333333334, 0.916)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=32)\n",
    "X_PCA_train = pca.fit_transform(X_train)\n",
    "X_PCA_test = pca.transform(X_test)\n",
    "clf = SVC(kernel=\"rbf\", C=14, gamma=0.001)#'scale')\n",
    "clf.fit(X_PCA_train, y_train)\n",
    "train_score = clf.score(X_PCA_train, y_train) \n",
    "test_score = clf.score(X_PCA_test, y_test)\n",
    "print(confusion_matrix(y_test, clf.predict(X_PCA_test)))\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[209  20]\n",
      " [ 21 250]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9926666666666667, 0.918)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=32, random_state=10)\n",
    "X_PCA_train = pca.fit_transform(X_train)\n",
    "X_PCA_test = pca.transform(X_test)\n",
    "clf = SVC(kernel=\"rbf\", C=11, gamma='scale')\n",
    "clf.fit(X_PCA_train, y_train)\n",
    "train_score = clf.score(X_PCA_train, y_train) \n",
    "test_score = clf.score(X_PCA_test, y_test)\n",
    "print(confusion_matrix(y_test, clf.predict(X_PCA_test)))\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X.shape[1] = 784 should be equal to 32, the number of features at training time",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-7d4dc32e54e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    497\u001b[0m         \"\"\"\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mndarray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \"\"\"\n\u001b[1;32m--> 333\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    484\u001b[0m             raise ValueError(\"X.shape[1] = %d should be equal to %d, \"\n\u001b[0;32m    485\u001b[0m                              \u001b[1;34m\"the number of features at training time\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m                              (X.shape[1], self.shape_fit_[1]))\n\u001b[0m\u001b[0;32m    487\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: X.shape[1] = 784 should be equal to 32, the number of features at training time"
     ]
    }
   ],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.874)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol = SVC(kernel=\"poly\", C=1000000, gamma=0.01, degree=3)\n",
    "pol.fit(X_train, y_train)\n",
    "train_score = pol.score(X_train, y_train) \n",
    "test_score = pol.score(X_test, y_test)\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SequentialFeatureSelector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-e1a5a7ae1162>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rbf\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#'scale')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequentialFeatureSelector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features_to_select\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SequentialFeatureSelector' is not defined"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel=\"rbf\", C=10, gamma=0.01)#'scale')\n",
    "sfs = SequentialFeatureSelector(clf, n_features_to_select=30)\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "train_score = sfs.score(X_train, y_train) \n",
    "test_score = sfs.score(X_test, y_test)\n",
    "print(confusion_matrix(y_test, sfs.predict(X_test)))\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The underlying estimator SVC has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-b5c32c80b250>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectFromModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_test_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     75\u001b[0m         X = check_array(X, dtype=None, accept_sparse='csr',\n\u001b[0;32m     76\u001b[0m                         force_all_finite=not tags.get('allow_nan', True))\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             warn(\"No features were selected: either the data is\"\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py\u001b[0m in \u001b[0;36mget_support\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0mare\u001b[0m \u001b[0mindices\u001b[0m \u001b[0minto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \"\"\"\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_support_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\u001b[0m in \u001b[0;36m_get_support_mask\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m                              \u001b[1;34m' \"prefit=True\" while passing the fitted'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                              ' estimator to the constructor.')\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_feature_importances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m         \u001b[0mthreshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_calculate_threshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\u001b[0m in \u001b[0;36m_get_feature_importances\u001b[1;34m(estimator, norm_order)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;34m\"`feature_importances_` attribute. Either pass a fitted estimator\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;34m\" to SelectFromModel or call fit before calling transform.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             % estimator.__class__.__name__)\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimportances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The underlying estimator SVC has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform."
     ]
    }
   ],
   "source": [
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_train_new = model.transform(X_train)\n",
    "X_test_new = model.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "116.08410113843281\n",
      "192.56446403612756\n",
      "233.1329517260497\n",
      "372.41837062244247\n",
      "498.0166480826465\n",
      "553.587930943828\n",
      "649.7354566077736\n",
      "703.1313348642916\n",
      "768.5151573747917\n",
      "829.0032639296237\n",
      "911.4231608643553\n",
      "943.616051357324\n",
      "1000.0807923059334\n",
      "1014.5923221843428\n",
      "1020.7763266513911\n",
      "976.6058122970758\n",
      "933.8379081576352\n",
      "870.1869505923703\n",
      "830.5325031224929\n",
      "786.1270853549755\n",
      "722.6528689377259\n",
      "630.4710296301578\n",
      "505.4231277572875\n",
      "437.99123947731755\n",
      "301.265245990404\n",
      "156.12924822344067\n",
      "0.0\n",
      "0.0\n",
      "207.38533304654374\n",
      "329.85340713753743\n",
      "453.64312911445245\n",
      "602.5161157293845\n",
      "728.451421657525\n",
      "836.3654474254272\n",
      "911.8164154367214\n",
      "985.2803974662932\n",
      "1035.9704903626512\n",
      "1108.4525145458147\n",
      "1166.487315851291\n",
      "1243.6767881517467\n",
      "1271.913438714071\n",
      "1299.904980231998\n",
      "1313.6195897593366\n",
      "1296.8149870449226\n",
      "1231.124575957381\n",
      "1177.9752051181865\n",
      "1151.5931965403104\n",
      "1084.9483301806342\n",
      "1007.3357413143466\n",
      "870.0702772425109\n",
      "756.1076241853107\n",
      "624.209977032085\n",
      "487.5947281769131\n",
      "324.1622527317333\n",
      "0.0\n",
      "0.0\n",
      "341.14100653255764\n",
      "550.8694584251629\n",
      "723.3391041470356\n",
      "815.1926658107351\n",
      "930.7209748594992\n",
      "1021.7832106656971\n",
      "1069.4075119671338\n",
      "1157.5738527879137\n",
      "1247.836059717616\n",
      "1287.0254292277136\n",
      "1353.5813575180775\n",
      "1412.8969972393231\n",
      "1431.0106373927965\n",
      "1431.956235764368\n",
      "1452.8483010917193\n",
      "1411.453205584693\n",
      "1365.1170336576715\n",
      "1305.1241672414149\n",
      "1299.2029874832633\n",
      "1215.9327702013625\n",
      "1124.8177606670297\n",
      "1023.0361818685537\n",
      "911.5276161163308\n",
      "793.3972265004745\n",
      "656.2081113967646\n",
      "483.1889228612545\n",
      "0.0\n",
      "0.0\n",
      "482.3455767827639\n",
      "760.3624630208454\n",
      "926.9041519739033\n",
      "1037.32234881241\n",
      "1081.8013421780006\n",
      "1135.8588163853935\n",
      "1201.279009428514\n",
      "1265.639743327283\n",
      "1318.706939056909\n",
      "1401.9395855003131\n",
      "1477.202587711436\n",
      "1507.147248991684\n",
      "1534.6373286671367\n",
      "1526.566241280994\n",
      "1530.5991350336888\n",
      "1511.5543741701022\n",
      "1474.8359331644733\n",
      "1439.127555228677\n",
      "1381.3775761919749\n",
      "1318.751275576784\n",
      "1239.6062104090024\n",
      "1142.981013762553\n",
      "1076.5278414056766\n",
      "1014.2855861020279\n",
      "877.1172420841859\n",
      "619.1554723993811\n",
      "0.0\n",
      "0.0\n",
      "614.7237296565324\n",
      "926.3149051036382\n",
      "1112.5808532621695\n",
      "1187.7210940433592\n",
      "1238.9654457944816\n",
      "1281.46187519318\n",
      "1322.020081625517\n",
      "1369.439266746221\n",
      "1417.8922943800685\n",
      "1494.1933352669353\n",
      "1551.4175708820671\n",
      "1594.6648703027952\n",
      "1616.0669453639307\n",
      "1604.9473793791492\n",
      "1597.6936419038266\n",
      "1598.8935731631543\n",
      "1581.654870729987\n",
      "1538.7412697439895\n",
      "1479.0208826984324\n",
      "1424.193513968407\n",
      "1362.2826448423511\n",
      "1292.595857693043\n",
      "1240.0524493665728\n",
      "1180.8406167708688\n",
      "1051.5646671668765\n",
      "755.0768227577139\n",
      "0.0\n",
      "0.0\n",
      "733.2845371352963\n",
      "1086.6091611177405\n",
      "1281.6096881526514\n",
      "1336.3621796005277\n",
      "1407.1048487222972\n",
      "1449.6064730896908\n",
      "1459.6517283835117\n",
      "1487.117230245254\n",
      "1548.3856092302012\n",
      "1597.0800039789985\n",
      "1676.82623325473\n",
      "1702.1714172038558\n",
      "1690.2895328454663\n",
      "1700.7838385517084\n",
      "1688.8299307017492\n",
      "1682.256242024909\n",
      "1664.1821534615235\n",
      "1645.046181507342\n",
      "1571.482038945771\n",
      "1525.2290789956314\n",
      "1485.2687134068417\n",
      "1429.2477412481987\n",
      "1407.0555926046668\n",
      "1334.3396166466036\n",
      "1194.376672811615\n",
      "902.9342252608953\n",
      "0.0\n",
      "0.0\n",
      "901.598476634335\n",
      "1257.3557031447722\n",
      "1429.8537544000083\n",
      "1479.7562510969433\n",
      "1538.9804910898108\n",
      "1569.5256668617576\n",
      "1618.0658375969863\n",
      "1648.3381817560744\n",
      "1662.8977045478714\n",
      "1708.7212700308216\n",
      "1762.3742661882288\n",
      "1776.6347975516112\n",
      "1792.4347845202594\n",
      "1793.4274604829573\n",
      "1782.5975263890543\n",
      "1763.9188819201977\n",
      "1752.6062208499304\n",
      "1720.4343193502866\n",
      "1701.8837265658467\n",
      "1641.5364802627328\n",
      "1615.0924639103282\n",
      "1565.9871288801953\n",
      "1511.1835326354044\n",
      "1462.9555003025164\n",
      "1351.5285931012043\n",
      "1032.8178215589037\n",
      "0.0\n",
      "0.0\n",
      "1077.3902865775779\n",
      "1415.466455363951\n",
      "1531.031595471907\n",
      "1589.5658591094555\n",
      "1632.3861846088248\n",
      "1679.606281477819\n",
      "1727.8799458415506\n",
      "1746.2103041093815\n",
      "1772.439703251565\n",
      "1806.2174298610698\n",
      "1830.0231608550018\n",
      "1842.5741634952922\n",
      "1853.2709868412728\n",
      "1855.5489730402703\n",
      "1846.1182506054115\n",
      "1844.4118504191424\n",
      "1813.9885451823554\n",
      "1800.7961571277908\n",
      "1772.9630245894614\n",
      "1749.5009741037343\n",
      "1710.4162969065403\n",
      "1672.9719833128247\n",
      "1628.8260699465732\n",
      "1573.4970348229551\n",
      "1468.2442018781471\n",
      "1113.028681109513\n",
      "0.0\n",
      "0.0\n",
      "1186.4243480765497\n",
      "1542.9552893726725\n",
      "1629.1475269435182\n",
      "1666.2919274876172\n",
      "1710.6875783118571\n",
      "1759.898794635581\n",
      "1808.0468104175684\n",
      "1816.4378731818056\n",
      "1845.9836712385047\n",
      "1859.7884214449045\n",
      "1862.3254312782497\n",
      "1859.0376609681532\n",
      "1857.2684819505528\n",
      "1862.6192922179682\n",
      "1860.7133565872357\n",
      "1859.7146782954746\n",
      "1855.1127640178572\n",
      "1848.1529070738245\n",
      "1832.5228652978965\n",
      "1816.5056031567303\n",
      "1785.5450437104487\n",
      "1723.9927120489806\n",
      "1708.1254948640687\n",
      "1675.7468642943113\n",
      "1578.2902625245965\n",
      "1223.3609073779385\n",
      "0.0\n",
      "0.0\n",
      "1319.6415691000807\n",
      "1639.4976139328487\n",
      "1691.1034548893897\n",
      "1731.2680608368844\n",
      "1779.8052329251752\n",
      "1825.287560888654\n",
      "1855.1069388919784\n",
      "1856.3206245796819\n",
      "1864.2973258925128\n",
      "1871.3473060069623\n",
      "1865.9469262909288\n",
      "1866.2280434318936\n",
      "1870.3676228815373\n",
      "1866.8541373139014\n",
      "1877.614139012961\n",
      "1873.0603362137622\n",
      "1866.942097964481\n",
      "1860.5127871922753\n",
      "1859.6585776359263\n",
      "1844.689140159208\n",
      "1829.255131680463\n",
      "1786.4252283882092\n",
      "1785.1007150180776\n",
      "1736.9807876275672\n",
      "1668.4810028816548\n",
      "1321.650714356751\n",
      "0.0\n",
      "0.0\n",
      "1366.5426018436508\n",
      "1735.8066029722013\n",
      "1746.8913007684519\n",
      "1771.3698804329251\n",
      "1791.4457519522168\n",
      "1833.661251077429\n",
      "1857.3554445932548\n",
      "1875.6819691184687\n",
      "1866.6712519385737\n",
      "1865.937249177941\n",
      "1861.4487586702667\n",
      "1869.5676728040683\n",
      "1875.6774374819613\n",
      "1859.2647035725993\n",
      "1864.0728418531307\n",
      "1864.589633480973\n",
      "1865.45055031416\n",
      "1871.938933321124\n",
      "1871.2537442581504\n",
      "1864.9266326004145\n",
      "1835.5214579442863\n",
      "1827.370266993674\n",
      "1797.9156026219187\n",
      "1774.0156075418402\n",
      "1743.4077675068606\n",
      "1413.5342029749002\n",
      "0.0\n",
      "0.0\n",
      "1473.5936552059366\n",
      "1793.76128296372\n",
      "1787.160284395633\n",
      "1790.1896656110248\n",
      "1817.6910968338082\n",
      "1841.9946873639665\n",
      "1864.4537854244402\n",
      "1878.0123568114202\n",
      "1860.6736173528561\n",
      "1858.5812241520653\n",
      "1853.3249641050527\n",
      "1850.8223250548092\n",
      "1861.4902783258774\n",
      "1864.2510549947167\n",
      "1863.3692661069024\n",
      "1862.3287450845496\n",
      "1869.0077614107097\n",
      "1865.2501367484074\n",
      "1873.9428986472035\n",
      "1865.3694597782385\n",
      "1847.1483963632293\n",
      "1832.0691961873417\n",
      "1785.7563989222342\n",
      "1782.6242576596842\n",
      "1783.206646004138\n",
      "1471.1270087505554\n",
      "0.0\n",
      "0.0\n",
      "1525.6240572964318\n",
      "1809.9796719384674\n",
      "1777.5516269999657\n",
      "1796.624439529103\n",
      "1832.2635550477912\n",
      "1863.1194475427394\n",
      "1870.1873517493111\n",
      "1866.468999181875\n",
      "1859.426215045759\n",
      "1849.210723484859\n",
      "1853.7960758332908\n",
      "1845.264565745885\n",
      "1859.4862252960918\n",
      "1855.3512337809523\n",
      "1859.3503075725148\n",
      "1857.920282378145\n",
      "1856.9337117660077\n",
      "1861.370767902248\n",
      "1870.7532527210253\n",
      "1866.3658489298052\n",
      "1842.989632548171\n",
      "1812.0578587882167\n",
      "1771.521336985373\n",
      "1777.8032168254547\n",
      "1790.771205889518\n",
      "1498.9956127156504\n",
      "0.0\n",
      "0.0\n",
      "1502.277744610886\n",
      "1794.345435807187\n",
      "1777.8155789156817\n",
      "1799.033747066635\n",
      "1840.9057470056432\n",
      "1845.6697752243608\n",
      "1853.5286709790234\n",
      "1859.0305969117894\n",
      "1848.0338157306096\n",
      "1839.7063108009947\n",
      "1837.4063160797177\n",
      "1829.2555842061745\n",
      "1842.6893233276753\n",
      "1843.6029334693765\n",
      "1844.3613509636716\n",
      "1853.630046731291\n",
      "1851.543520638568\n",
      "1862.7383478719087\n",
      "1864.150293951183\n",
      "1864.6888612840878\n",
      "1852.223010974719\n",
      "1821.064132041765\n",
      "1778.2585753933822\n",
      "1761.4240732584274\n",
      "1743.5705995195055\n",
      "1424.9512122606736\n",
      "0.0\n",
      "0.0\n",
      "1408.0947837521144\n",
      "1735.1742095745844\n",
      "1795.4989072855249\n",
      "1802.8595835428018\n",
      "1832.1538587557825\n",
      "1858.7184684605766\n",
      "1865.0596567800405\n",
      "1864.9158248955096\n",
      "1846.6077950870304\n",
      "1836.474180473593\n",
      "1842.3789841123987\n",
      "1836.7216373027782\n",
      "1843.1214527187587\n",
      "1844.962709418379\n",
      "1838.3480676002698\n",
      "1841.2053581639054\n",
      "1843.1154632448054\n",
      "1850.6527280090995\n",
      "1862.838578449626\n",
      "1865.1246932445033\n",
      "1837.4524462414731\n",
      "1808.7061527106769\n",
      "1766.459857932546\n",
      "1721.2883397288717\n",
      "1655.8640404875805\n",
      "1305.3966605976595\n",
      "0.0\n",
      "0.0\n",
      "1273.305994823271\n",
      "1657.6630002119223\n",
      "1750.980949822308\n",
      "1791.758999562408\n",
      "1832.5883742947879\n",
      "1854.3136143890672\n",
      "1860.146470729252\n",
      "1858.5934395404402\n",
      "1851.604744310302\n",
      "1847.204486750785\n",
      "1844.7119751532962\n",
      "1831.8259554465671\n",
      "1838.2476905316487\n",
      "1846.6159907603499\n",
      "1849.9515364435945\n",
      "1851.7250104018328\n",
      "1858.5301833835865\n",
      "1858.071497811387\n",
      "1859.102583238104\n",
      "1864.4405079218002\n",
      "1818.687468068727\n",
      "1786.231931739943\n",
      "1746.615649316479\n",
      "1673.6231761785964\n",
      "1557.339085851663\n",
      "1142.3530589947652\n",
      "0.0\n",
      "0.0\n",
      "1139.1741110805954\n",
      "1540.9958528934822\n",
      "1660.9277811650388\n",
      "1722.0926594104035\n",
      "1789.1882931364978\n",
      "1838.3446950749337\n",
      "1857.7645495908898\n",
      "1873.4044201128354\n",
      "1863.014443341442\n",
      "1861.2348154684573\n",
      "1857.6110603323984\n",
      "1855.6853904797265\n",
      "1852.340057023051\n",
      "1858.0187624977882\n",
      "1855.1297709012827\n",
      "1863.6276862865627\n",
      "1852.4580062828045\n",
      "1857.681628640127\n",
      "1837.331007461288\n",
      "1830.8422637761266\n",
      "1798.4057075784528\n",
      "1737.6114999291879\n",
      "1675.3417939431085\n",
      "1606.6918383403888\n",
      "1383.5906454262818\n",
      "1004.0884741185048\n",
      "0.0\n",
      "0.0\n",
      "1042.0509406785845\n",
      "1390.1973319641675\n",
      "1546.2243713944479\n",
      "1626.1658591545784\n",
      "1723.356964549366\n",
      "1798.3951439362522\n",
      "1825.5446761938192\n",
      "1843.6381935394827\n",
      "1858.3029229533695\n",
      "1862.0316828772816\n",
      "1854.0702973457742\n",
      "1866.9731043524785\n",
      "1861.355580329667\n",
      "1863.3634694939744\n",
      "1869.1988731220704\n",
      "1868.5957195637604\n",
      "1858.4166808884465\n",
      "1868.2075150332894\n",
      "1832.8456549527682\n",
      "1792.4174946299488\n",
      "1746.2788803595665\n",
      "1655.3972354777072\n",
      "1564.3589255283293\n",
      "1455.8503685522046\n",
      "1240.4953669981815\n",
      "863.2807866111548\n",
      "0.0\n",
      "0.0\n",
      "922.8592724125831\n",
      "1266.6703990520753\n",
      "1448.682891108193\n",
      "1558.482290631284\n",
      "1638.227497745214\n",
      "1738.495752175369\n",
      "1782.20962408448\n",
      "1827.7083337703848\n",
      "1848.1059276521435\n",
      "1862.991834437379\n",
      "1869.0959954892735\n",
      "1876.8147014018589\n",
      "1873.678218679997\n",
      "1873.594952595515\n",
      "1859.6283534432469\n",
      "1873.2972838313333\n",
      "1827.0773767404637\n",
      "1820.1585879824038\n",
      "1793.7657321614258\n",
      "1735.3698071050653\n",
      "1669.316514731234\n",
      "1577.9102520561923\n",
      "1428.677761500851\n",
      "1289.041607965151\n",
      "1086.0107670233601\n",
      "788.8710442596812\n",
      "0.0\n",
      "0.0\n",
      "779.0382951073639\n",
      "1134.950324575505\n",
      "1328.2651149638818\n",
      "1431.6623765673385\n",
      "1560.729198378949\n",
      "1668.3938234169214\n",
      "1730.4412889728496\n",
      "1782.1250284402824\n",
      "1810.9405803174323\n",
      "1857.1342120275945\n",
      "1855.9121336188423\n",
      "1849.2823438035339\n",
      "1859.4929773781587\n",
      "1845.913395046357\n",
      "1840.666593722342\n",
      "1839.721573174008\n",
      "1792.7356098291618\n",
      "1742.0951696781653\n",
      "1729.0587027381373\n",
      "1665.68450365738\n",
      "1557.2656369478254\n",
      "1452.1225986646805\n",
      "1277.5482538994622\n",
      "1141.5231381465624\n",
      "967.3362860154433\n",
      "704.3306344184374\n",
      "0.0\n",
      "0.0\n",
      "693.3385102879988\n",
      "991.1082147272239\n",
      "1185.6838994595971\n",
      "1299.4468242440119\n",
      "1466.716845072134\n",
      "1584.5139228443163\n",
      "1648.7275417993687\n",
      "1700.7886323174537\n",
      "1770.0463241668306\n",
      "1813.6692473708229\n",
      "1813.2044987645447\n",
      "1821.491928672252\n",
      "1812.599044649195\n",
      "1788.3640586707809\n",
      "1795.9945156270974\n",
      "1758.7267097712397\n",
      "1706.1599816272649\n",
      "1649.6139625138405\n",
      "1607.7403612008939\n",
      "1519.9270828228734\n",
      "1427.5712359746713\n",
      "1294.0791442968525\n",
      "1114.4436445083084\n",
      "1014.2691979638594\n",
      "851.314794031545\n",
      "614.7693188076667\n",
      "0.0\n",
      "0.0\n",
      "577.1104092245283\n",
      "849.0194066824085\n",
      "1021.3984972353969\n",
      "1177.1855622506773\n",
      "1338.1553658664398\n",
      "1442.916326075022\n",
      "1531.165813280522\n",
      "1596.6265304581125\n",
      "1682.7679122083307\n",
      "1718.1284886306596\n",
      "1744.3888101235543\n",
      "1761.9016473516167\n",
      "1755.2044715763996\n",
      "1706.5086244197646\n",
      "1724.0070587844784\n",
      "1702.7526912569003\n",
      "1645.617492448169\n",
      "1545.3132699114046\n",
      "1465.0892405942723\n",
      "1355.2342449583932\n",
      "1269.327837213407\n",
      "1111.3605410263121\n",
      "976.3053283614549\n",
      "839.4621882685425\n",
      "690.1291212967005\n",
      "481.2582920695351\n",
      "0.0\n",
      "0.0\n",
      "458.5603105313224\n",
      "680.897698996113\n",
      "874.6335305527481\n",
      "1004.5991296160291\n",
      "1144.2293611287823\n",
      "1322.3354198225418\n",
      "1400.1280363248331\n",
      "1496.816993622159\n",
      "1584.1144327059244\n",
      "1638.6474203383516\n",
      "1649.692313468708\n",
      "1673.5749212095811\n",
      "1675.7768443864002\n",
      "1646.306458122933\n",
      "1622.7840812362963\n",
      "1596.3803363491834\n",
      "1512.1907367039632\n",
      "1419.3957205173215\n",
      "1335.6120768947037\n",
      "1228.3399837910767\n",
      "1132.380534582377\n",
      "975.5369971544113\n",
      "840.1113474057644\n",
      "724.2421244426171\n",
      "563.2326238604747\n",
      "360.09505519301734\n",
      "0.0\n",
      "0.0\n",
      "329.88356714500765\n",
      "452.72891229450835\n",
      "653.5217645027121\n",
      "813.943609612898\n",
      "997.7522129091617\n",
      "1175.395561826079\n",
      "1285.1715820160034\n",
      "1370.578001254363\n",
      "1483.479387438498\n",
      "1556.5400353823798\n",
      "1593.8364290690793\n",
      "1610.998263149321\n",
      "1606.1128834750612\n",
      "1600.7107609140019\n",
      "1568.9440867082462\n",
      "1501.3821189537693\n",
      "1395.1889278159938\n",
      "1306.4806990262734\n",
      "1229.869907523296\n",
      "1101.7784047328125\n",
      "950.6445937866016\n",
      "808.1004594320736\n",
      "697.9968712559306\n",
      "554.9886902054594\n",
      "345.90313870216164\n",
      "201.8386935120028\n",
      "0.0\n",
      "0.0\n",
      "242.231517464695\n",
      "313.1106958897296\n",
      "419.60858070123777\n",
      "619.7979805635523\n",
      "783.1992656827008\n",
      "918.7808241699995\n",
      "1083.6623348994858\n",
      "1214.7109063307057\n",
      "1328.5098239284898\n",
      "1412.2542431428305\n",
      "1509.3270642029843\n",
      "1562.8719569899363\n",
      "1570.0076271267412\n",
      "1535.9895648061781\n",
      "1463.8299218062612\n",
      "1387.9471407825508\n",
      "1264.0637590934132\n",
      "1147.5718576782037\n",
      "1049.3327775900514\n",
      "891.4504900202211\n",
      "768.6242917344298\n",
      "644.4418336453582\n",
      "507.7321900173794\n",
      "381.7731225462373\n",
      "233.4204029560361\n",
      "160.32670575872416\n",
      "0.0\n",
      "0.0\n",
      "100.51555406584922\n",
      "195.64649098597755\n",
      "244.68589056155514\n",
      "389.8429513292238\n",
      "518.6064402846423\n",
      "617.3067520961476\n",
      "728.1336032523759\n",
      "887.1272708868447\n",
      "967.9934507672908\n",
      "1050.9158206070538\n",
      "1169.798686253476\n",
      "1246.0053840522548\n",
      "1239.334956156575\n",
      "1212.8648866274498\n",
      "1120.8724796265328\n",
      "1037.3516128131328\n",
      "934.4158474635817\n",
      "837.6832551925785\n",
      "699.9691778453829\n",
      "586.1159098353131\n",
      "514.6807505122905\n",
      "422.5986263307024\n",
      "365.6133550737688\n",
      "296.86926655685767\n",
      "123.04618289359436\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(784):\n",
    "    print(sum(abs(X.T[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0       1       2       3       4       5       6       7       8    \\\n",
       "count  2000.0  2000.0  2000.0  2000.0  2000.0  2000.0  2000.0  2000.0  2000.0   \n",
       "mean      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "std       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "min       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "50%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "75%       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "max       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "          9    ...     774     775     776     777     778     779     780  \\\n",
       "count  2000.0  ...  2000.0  2000.0  2000.0  2000.0  2000.0  2000.0  2000.0   \n",
       "mean      0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "std       0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "min       0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "25%       0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "50%       0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "75%       0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "max       0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "          781     782     783  \n",
       "count  2000.0  2000.0  2000.0  \n",
       "mean      0.0     0.0     0.0  \n",
       "std       0.0     0.0     0.0  \n",
       "min       0.0     0.0     0.0  \n",
       "25%       0.0     0.0     0.0  \n",
       "50%       0.0     0.0     0.0  \n",
       "75%       0.0     0.0     0.0  \n",
       "max       0.0     0.0     0.0  \n",
       "\n",
       "[8 rows x 784 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_describe = pd.DataFrame(X)\n",
    "df_describe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[158  71]\n",
      " [ 55 216]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.918, 0.748)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "# DECISION TREE \n",
    "max_depth = 6\n",
    "clf = DecisionTreeClassifier(max_depth=max_depth).fit(X_train, y_train)\n",
    "train_score = clf.score(X_train, y_train) \n",
    "test_score = clf.score(X_test, y_test)\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[182  47]\n",
      " [ 41 230]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9346666666666666, 0.824)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DECISION TREE WITH PCA\n",
    "max_depth = 6\n",
    "n_components = 30\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_PCA_train = pca.fit_transform(X_train)\n",
    "X_PCA_test = pca.transform(X_test)\n",
    "clf = DecisionTreeClassifier(max_depth=max_depth).fit(X_PCA_train, y_train)\n",
    "train_score = clf.score(X_PCA_train, y_train) \n",
    "test_score = clf.score(X_PCA_test, y_test)\n",
    "print(confusion_matrix(y_test, clf.predict(X_PCA_test)))\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 15\n",
    "\n",
    "X_ant = np.load('G_3_ant.npy')\n",
    "n_ant = X_ant.shape[0]\n",
    "y_ant = np.ones(n_ant)\n",
    "X_grapes = np.load('G_3_grapes.npy')\n",
    "n_grapes = X_ant.shape[0]\n",
    "y_grapes = np.zeros(n_grapes)\n",
    "X_raw = np.vstack([X_ant, X_grapes])\n",
    "y = np.hstack([y_ant, y_grapes])\n",
    "X, y = shuffle(X_raw, y, random_state=None)\n",
    "X[np.where(X<eps)] = 0  # ignore little points, small pixels\n",
    "X = X / 255.\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(30, (5, 5), input_shape=s[1:], activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    opt = optimizers.Adam(lr=0.0004, decay=1e-6)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1340, 28, 28, 1) 2\n"
     ]
    }
   ],
   "source": [
    "# one hot encode outputs\n",
    "y_train_cnn = to_categorical(y_train)\n",
    "y_test_cnn = to_categorical(y_test)\n",
    "num_classes = y_test_cnn.shape[1]\n",
    "\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train_cnn = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "X_test_cnn = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')\n",
    "s = X_train_cnn.shape\n",
    "print(s, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1340 samples, validate on 660 samples\n",
      "Epoch 1/12\n",
      "1340/1340 [==============================] - 1s 818us/step - loss: 0.4061 - accuracy: 0.8231 - val_loss: 0.3690 - val_accuracy: 0.8455\n",
      "Epoch 2/12\n",
      "1340/1340 [==============================] - 1s 659us/step - loss: 0.2955 - accuracy: 0.8813 - val_loss: 0.3068 - val_accuracy: 0.8803\n",
      "Epoch 3/12\n",
      "1340/1340 [==============================] - 1s 657us/step - loss: 0.2696 - accuracy: 0.8881 - val_loss: 0.2744 - val_accuracy: 0.8970\n",
      "Epoch 4/12\n",
      "1340/1340 [==============================] - 1s 635us/step - loss: 0.2193 - accuracy: 0.9157 - val_loss: 0.2574 - val_accuracy: 0.9061\n",
      "Epoch 5/12\n",
      "1340/1340 [==============================] - 1s 615us/step - loss: 0.1901 - accuracy: 0.9299 - val_loss: 0.2653 - val_accuracy: 0.8909\n",
      "Epoch 6/12\n",
      "1340/1340 [==============================] - 1s 615us/step - loss: 0.1726 - accuracy: 0.9336 - val_loss: 0.2591 - val_accuracy: 0.8924\n",
      "Epoch 7/12\n",
      "1340/1340 [==============================] - 1s 635us/step - loss: 0.1361 - accuracy: 0.9478 - val_loss: 0.1981 - val_accuracy: 0.9197\n",
      "Epoch 8/12\n",
      "1340/1340 [==============================] - 1s 672us/step - loss: 0.1117 - accuracy: 0.9612 - val_loss: 0.2280 - val_accuracy: 0.9182\n",
      "Epoch 9/12\n",
      "1340/1340 [==============================] - 1s 635us/step - loss: 0.0992 - accuracy: 0.9575 - val_loss: 0.2192 - val_accuracy: 0.9091\n",
      "Epoch 10/12\n",
      "1340/1340 [==============================] - 1s 639us/step - loss: 0.0954 - accuracy: 0.9642 - val_loss: 0.2153 - val_accuracy: 0.9197\n",
      "Epoch 11/12\n",
      "1340/1340 [==============================] - 1s 632us/step - loss: 0.0814 - accuracy: 0.9649 - val_loss: 0.2261 - val_accuracy: 0.9182\n",
      "Epoch 12/12\n",
      "1340/1340 [==============================] - 1s 666us/step - loss: 0.0717 - accuracy: 0.9701 - val_loss: 0.2466 - val_accuracy: 0.9045\n",
      "Final CNN accuracy:  90.45454263687134 %\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "epochs = 12\n",
    "\n",
    "# build the model\n",
    "model = cnn_model()\n",
    "# Fit the model\n",
    "history = model.fit(X_train_cnn, y_train_cnn,\n",
    "                    validation_data=(X_test_cnn, y_test_cnn),\n",
    "                    epochs=epochs, batch_size=batch_size)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
    "print('Final CNN accuracy: ', scores[1]*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 64 candidates, totalling 192 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=-1)]: Done 192 out of 192 | elapsed: 13.1min finished\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KerasClassifier' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-305d17e29b7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                               cv=3)\n\u001b[0;32m      8\u001b[0m \u001b[0mmlp_best_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_cnn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_cnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_cnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_cnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp_best_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp_best_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KerasClassifier' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "grid = {'batch_size': [2, 4, 8, 12, 16, 18, 20, 22],\n",
    "        'epochs': [2, 4, 8, 12, 16, 18, 20, 22]}\n",
    "model = KerasClassifier(build_fn=cnn_model, verbose=0)\n",
    "mlp_best_param = GridSearchCV(estimator=model,\n",
    "                              param_grid=grid,\n",
    "                              n_jobs=-1, verbose=3,\n",
    "                              cv=5)\n",
    "mlp_best_param.fit(X_train_cnn,y_train_cnn)\n",
    "print(mlp_best_param.best_score_)\n",
    "print(mlp_best_param.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.917154053846995\n",
      "{'batch_size': 8, 'epochs': 22}\n"
     ]
    }
   ],
   "source": [
    "print(mlp_best_param.best_score_)\n",
    "print(mlp_best_param.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
